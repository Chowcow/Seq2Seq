{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook will introduce a very simple seq2seq model that takes a text input (a book) and outputs text given that input. The goal of this is to use the knowledge gained here to build a Q&A database generation tool where a user inputs an FAQ and questions are produced from that. I chose to submit this preliminary model as I will not have the latter model complete in time. A number of blog posts cover Seq2Seq models, with the most popular use case for them being translation tools.\n",
    "\n",
    "## Note\n",
    "\n",
    "The model performs very poorly as I did not provide it with enough data. That being said, it should run on any conventional notebook in under a few minutes.\n",
    "\n",
    "References:\n",
    "https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1\n",
    "https://github.com/OpenNMT/OpenNMT-tf/blob/master/README.md\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the default tensor type at the top\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else \n",
    "                              torch.FloatTensor)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144348\n"
     ]
    }
   ],
   "source": [
    "#Import text data, Alice in Wonderland from local directory\n",
    "path = \"./aiw.txt\"\n",
    "\n",
    "text= open(path).read()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, ‘and what is the use of a book,’ thought Alice ‘without pictures or\\nconversations?’\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain w'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The vocabulary is all the unique symbols used in the text. This is the benefit of \n",
    "working with a character level RNN.\"\"\"\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size= len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " ':': 9,\n",
       " ';': 10,\n",
       " '?': 11,\n",
       " 'A': 12,\n",
       " 'B': 13,\n",
       " 'C': 14,\n",
       " 'D': 15,\n",
       " 'E': 16,\n",
       " 'F': 17,\n",
       " 'G': 18,\n",
       " 'H': 19,\n",
       " 'I': 20,\n",
       " 'J': 21,\n",
       " 'K': 22,\n",
       " 'L': 23,\n",
       " 'M': 24,\n",
       " 'N': 25,\n",
       " 'O': 26,\n",
       " 'P': 27,\n",
       " 'Q': 28,\n",
       " 'R': 29,\n",
       " 'S': 30,\n",
       " 'T': 31,\n",
       " 'U': 32,\n",
       " 'V': 33,\n",
       " 'W': 34,\n",
       " 'X': 35,\n",
       " 'Y': 36,\n",
       " 'Z': 37,\n",
       " '[': 38,\n",
       " ']': 39,\n",
       " '_': 40,\n",
       " 'a': 41,\n",
       " 'b': 42,\n",
       " 'c': 43,\n",
       " 'd': 44,\n",
       " 'e': 45,\n",
       " 'f': 46,\n",
       " 'g': 47,\n",
       " 'h': 48,\n",
       " 'i': 49,\n",
       " 'j': 50,\n",
       " 'k': 51,\n",
       " 'l': 52,\n",
       " 'm': 53,\n",
       " 'n': 54,\n",
       " 'o': 55,\n",
       " 'p': 56,\n",
       " 'q': 57,\n",
       " 'r': 58,\n",
       " 's': 59,\n",
       " 't': 60,\n",
       " 'u': 61,\n",
       " 'v': 62,\n",
       " 'w': 63,\n",
       " 'x': 64,\n",
       " 'y': 65,\n",
       " 'z': 66,\n",
       " '‘': 67,\n",
       " '’': 68,\n",
       " '“': 69,\n",
       " '”': 70}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{c:i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionaries from character --> index and index --> character\n",
    "c_to_idx= {c:i for i, c in enumerate(chars)}\n",
    "idx_to_c= {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 19, 12, 27, 31, 16, 29, 1, 20, 8]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Convert whole text to indicies. Want each character to be \n",
    "represented by its index in the vocabulary. This is how we will feed to RNN'''\n",
    "\n",
    "text_idx = [c_to_idx[c] for c in text]\n",
    "text_len = len(text_idx)\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on t\n",
      "--------\n",
      "t-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on t\n"
     ]
    }
   ],
   "source": [
    "#Check it works to convert back : join up the indicies\n",
    "\n",
    "print(text[25:100])\n",
    "print(\"--------\")\n",
    "print(''.join([idx_to_c[i] for i in text_idx[25:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataLoader\n",
    "#Sequence of characters passed to RNN at a time. This dictates the length of the unrolled model (#timesteps)\n",
    "#Batch size affects splitting of raw data as well as model architecture\n",
    "\n",
    "seq_len = 8\n",
    "batch_size= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wnat a non-overlapping set of inputs and outputs. Each X should be equal to the sequence length, while the Y, shifted by 1. Note that we don't go to the end for Y.\n",
    "\n",
    "idx_in_data = [text_idx[idx:idx+seq_len] for idx in range(0, text_len-1-seq_len,seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18043, 8)\n",
      "[[14 19 12 27 31 16 29  1]\n",
      " [20  8  1 15 55 63 54  1]\n",
      " [60 48 45  1 29 41 42 42]]\n"
     ]
    }
   ],
   "source": [
    "#Convert these inputs into a numpy array and provide info. Note dimensions are the total number of sequences in the corpus and the sequence length.\n",
    "\n",
    "inp = np.array(idx_in_data)\n",
    "print(inp.shape)\n",
    "print(inp[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the samething with Y\n",
    "\n",
    "idx_out_data = [text_idx[idx:idx+seq_len] for idx in range(1, text_len-seq_len, seq_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18043, 8)\n",
      "[[19 12 27 31 16 29  1 20]\n",
      " [ 8  1 15 55 63 54  1 60]\n",
      " [48 45  1 29 41 42 42 49]]\n"
     ]
    }
   ],
   "source": [
    "#Confirm that the target array is the input array shifted by 1. We'll be predicting the next character in sequence.\n",
    "\n",
    "outp = np.array(idx_out_data)\n",
    "print(outp.shape)\n",
    "print(outp[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split up the input and target data into training and test sets.\n",
    "Return 4 numpy arrays- training input, training targets, test input, and test targets'''\n",
    "\n",
    "def train_test_split(inp_data, out_data, train_fraction):\n",
    "    trn_idx = np.random.rand(len(inp_data)) < train_fraction\n",
    "    \n",
    "    inp_trn = inp_data[trn_idx]\n",
    "    inp_test = inp_data[~trn_idx]\n",
    "    \n",
    "    outp_trn= out_data[trn_idx]\n",
    "    outp_test= out_data[~trn_idx]\n",
    "    return inp_trn, outp_trn, inp_test, outp_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 90%training, 10% test. This ratio should be bigger with a larger corpus.\n",
    "\n",
    "x_trn, y_trn, x_val, y_val = train_test_split(inp,outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PyTorch Dataset class for character level text generation. X and Y have widths equal to the sequence length'''\n",
    "\n",
    "class CharSeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        return(item,label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and validation datasets\n",
    "\n",
    "train_ds = CharSeqDataset(x_trn, y_trn)\n",
    "val_ds = CharSeqDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn these into PyTorch dataloaders with batch size = bath_size.\n",
    "#This will take care of the shuffling and batching\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size = batch_size, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple experiments with Data Loaders:\n",
    "1. The X and Y values are paired. Show that shuffling keeps them lined up.\n",
    "2. You get a different order whenever you iterate over a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[52, 46,  8,  1, 20, 53, 41, 47],\n",
      "        [58,  1, 45, 65, 45, 59,  7,  7]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[46,  8,  1, 20, 53, 41, 47, 49],\n",
      "        [ 1, 45, 65, 45, 59,  7,  7, 41]])\n"
     ]
    }
   ],
   "source": [
    "#Exp 1\n",
    "exp_iter= iter(train_dl)\n",
    "x_exp, y_exp = next(exp_iter)\n",
    "\n",
    "#Exp 1\n",
    "print(x_exp.shape) # batch size by sequence length\n",
    "print(type(x_exp))\n",
    "print(x_exp[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp.shape)\n",
    "print(type(y_exp))\n",
    "print(y_exp[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[63, 49, 60, 48,  1, 60, 48, 45],\n",
      "        [53, 55, 61, 59, 45,  6,  1, 41]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[49, 60, 48,  1, 60, 48, 45,  0],\n",
      "        [55, 61, 59, 45,  6,  1, 41, 46]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 2.\n",
    "exp_iter2 = iter(train_dl)\n",
    "x_exp2, y_exp2 = next(exp_iter2)\n",
    "\n",
    "print(x_exp2.shape) # batch size by sequence length\n",
    "print(type(x_exp2))\n",
    "print(x_exp2[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp2.shape)\n",
    "print(type(y_exp2))\n",
    "print(y_exp2[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN model class using PyTorch\n",
    "\n",
    "\n",
    "Pytorch model.\n",
    "One sequence step involves embedding layer->RNN->fully connected layer->softmax over vocabulary\n",
    "A couple tricky points:\n",
    "-Want to keep the hidden activation values after a forward pass. So I have to detach h after a \n",
    "forward pass so BPTT doesn't have to go through all the steps back to the very beginning of the corpus.\n",
    "-Output predictions are rank 3 tensor of batch_size x seq_len x vocab length (it's a prediction over the vocab\n",
    "for each char in the sequence and for each sequence in the minibatch). Softmax only accepts rank 2, so need to\n",
    "reshape this into a (batch_size * seq_len) x vocab_length tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension for Character's learned embeddings. Number of hidden units in the RNN.\n",
    "emb_dim= 42\n",
    "n_hidden = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim) # Going from vocab size down to embedding size\n",
    "        # Automatically runs for N sequence steps, which is known from input data size\n",
    "        self.rnn = nn.RNN(emb_dim, n_hidden) # embedding size to number of hidden units\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.h = self.init_h(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.h = self.init_h(bs)\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp, h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data) # Save hidden values for next forward pass. Remove from BPTT by rewrapping in Var\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_h(self, bs):\n",
    "        return Variable(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function does 1 epoch (pass through the data)\n",
    "def train(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        targets = targets.view(-1).to(device)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calculates average loss over all the test data.\n",
    "def test(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.to(device))\n",
    "            targets = targets.view(-1).to(device)\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning rate was chosen as recommended. I'm sure I could optimize it more given more time.\n",
    "\n",
    "model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 4, Training Loss: 2.9062, Validation Loss: 2.8576\n",
      "Epoch: 2 / 4, Training Loss: 2.6239, Validation Loss: 2.5797\n",
      "Epoch: 3 / 4, Training Loss: 2.5064, Validation Loss: 2.4283\n",
      "Epoch: 4 / 4, Training Loss: 2.3775, Validation Loss: 2.3350\n"
     ]
    }
   ],
   "source": [
    "#I'm running this on a local computer, so I rather keep the epochs low and use the model as a PoC.\n",
    "\n",
    "epochs = 4\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(model, optimizer, criterion, train_dl)\n",
    "    test_loss = test(model, val_dl, criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given an input and a trained model, do a forward pass and predict the next character in the input sequence.\n",
    "Return this character as its integer index in the vocabulary.\n",
    "'''\n",
    "def next_letter(my_model, inp):\n",
    "\n",
    "    inp = torch.tensor([inp])\n",
    "    model_out = my_model(inp)\n",
    "    # Grab the last letter from the model output\n",
    "    # And sample from the vocabulary based on the weighted probability for character in the vocab.\n",
    "    # This makes this result non-deterministic, there can be variance between the next letter in the sequence\n",
    "    # depending on the sampling. Especially if multiple character get assigned similar probabilities.\n",
    "    next_idx = torch.multinomial(model_out[-1].exp(), 1).item()\n",
    "    \n",
    "    # return the next character index in the sequence\n",
    "    return next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 B\n"
     ]
    }
   ],
   "source": [
    "mytext = \"thos\"\n",
    "mytext = [c_to_idx[i] for i in mytext]\n",
    "nl = next_letter(model, mytext)\n",
    "print(nl, idx_to_c[nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Keep generating the next character in the sequence. Repeatedly move the sampling window to include the latest\n",
    "prediction and predict the next letter. Goes for num_chars repetitions.\n",
    "'''\n",
    "def gen_text(my_model, inp, num_chars):\n",
    "    text = inp\n",
    "    inp = [c_to_idx[i] for i in inp]\n",
    "    for c in range(num_chars):\n",
    "        l = next_letter(my_model, inp)\n",
    "        text += idx_to_c[l]\n",
    "        inp = inp[1:]+[l]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloint--ove? ‘YVNam!’\n",
      "\n",
      "ures il\n",
      "ore. of en:a dot Allaryntow, madtas ce, and thir thu fadliec it af che cit ous of thedriter ab\n",
      "t:e. ans bto he ca Iinp whas oilgeet bertehyftoting wals teot. ‘Sy!’’\n",
      "\n",
      "‘I\n",
      "B Thery eupnf her’t anr herome\n",
      "\n",
      "‘Thit,’ver,AKit saidl!!: l, pthe Heare is ireen -h*o to agrin.\n",
      "‘wNoutfomu gor\n",
      "ucbynbin\n",
      "tha beab atosaingit Alihk I’ sy-e’ shatleaot, ail dfon  ailas  are onne toibt, qouse\n"
     ]
    }
   ],
   "source": [
    "gen_text(model, \"Hello\", 400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Summary\n",
    "It looks like it's starting to work alright, especially since it hasn't trained for too long. I found that benefits of continued training started to level off after ~30 epochs or so.\n",
    "\n",
    "I learned something important here though. When I split up the corpus into sequences of length 8 (sequence length / bptt length), characters 1 - 8 are the first training example in batch 1, 9 - 16 are the second etc. What that means is that the hidden states after the forward pass are meaningless for the next batch. There's no information gained about the previous sequence to help you out with the current sequence!\n",
    "\n",
    "Here's a different idea -> What if characters 1 - 8 make up the first training example of the first batch, then characters 9 - 16 make up the first training example of the second batch. That way (since we're saving activation values) when character 9 gets passed in as the first step to the RNN, the activations correspond to what came out after character 8, which was the last character of example 1 in the previous minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data up into \"vertical stacks\" as explained above\n",
    "\n",
    "Want to split the corpus into a number of chunks equal to the number of mini batches (512) because each chunk will represent a row example in successive minibatches. Also, the sequences need to still be seq_len long. So it's easiest to figure out how long (number of chars) a block can be if we need to get 512 into the corpus, then round that length to something evenly divisible by the sequene length. We lose a little bit of potential information, but it MAY be easier than having the final minibatch have a shorter sequence. Another option would be to zero-pad that last sequence.. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 144348\n",
      "Batch size: 512\n",
      "Sequence length / bptt length: 8\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(text)}')\n",
    "print(f'Batch size: {batch_size}')\n",
    "print(f'Sequence length / bptt length: {seq_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "-Pass in text, batch size, and sequence length to get back numpy array where consecutive text is lined up\n",
    "across minibatches.\n",
    "-Remember, in a list comprehension, the second for executes fully (it's the nested one). Each pass through the text, \n",
    "we grab a sequence lengthed bit of text from each \"mini-batch block\". Then next pass the index is shifted over.\n",
    "The idea is that you build an array where mini_batch example i makes continuous text across the mini batches,\n",
    "rather than within a minibatch.\n",
    "'''\n",
    "def vertical_chunk(text, bs, sl):\n",
    "    s_per_block = len(text) // sl // bs\n",
    "    c_per_block = s_per_block * sl\n",
    "    tl = c_per_block * bs\n",
    "    \n",
    "    r = [text[b+i : b+i+sl] for i in range(0,c_per_block,sl) for b in range(0,tl,c_per_block)]\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_inp = vertical_chunk(text_idx, batch_size, seq_len)\n",
    "stacked_outp = vertical_chunk(text_idx[1:], batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17920, 8)\n",
      "(17920, 8)\n"
     ]
    }
   ],
   "source": [
    "print(stacked_inp.shape)\n",
    "print(stacked_outp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 19, 12, 27, 31, 16, 29, 1, 20, 8, 1, 15, 55, 63, 54, 1]\n",
      "********\n",
      "[14 19 12 27 31 16 29  1]\n",
      "[19 12 27 31 16 29  1 20]\n",
      "********\n",
      "[20  8  1 15 55 63 54  1]\n",
      "[ 8  1 15 55 63 54  1 60]\n"
     ]
    }
   ],
   "source": [
    "# Show that continuous text is split over minibatch indices\n",
    "print(text_idx[:16])\n",
    "print(\"********\")\n",
    "print(stacked_inp[0])\n",
    "print(stacked_outp[0])\n",
    "print(\"********\")\n",
    "print(stacked_inp[512])\n",
    "print(stacked_outp[512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a dataloader. But we don't want to shuffle the data, because the continuity is important for the activations. So just split up the data into test and train by index, then make a dataloader without shuffle on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't want to randomly split. Just take the first half\n",
    "# st_x_trn, st_y_trn, st_x_val, st_y_val = train_test_split(stacked_inp, stacked_outp, 0.9)\n",
    "def data_split_nonrandom(in_data, out_data, train_frac):\n",
    "    portion = int(len(in_data) * train_frac)\n",
    "    return in_data[:portion], out_data[:portion], in_data[portion:], out_data[portion:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_x_trn, st_y_trn, st_x_val, st_y_val = data_split_nonrandom(stacked_inp, stacked_outp, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "st_train_ds = CharSeqDataset(st_x_trn, st_y_trn)\n",
    "st_val_ds = CharSeqDataset(st_x_val, st_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unshuffled dataloaders\n",
    "# Is there a way to shuffle cross batch? Does this even have a point?\n",
    "st_train_dl = DataLoader(dataset=st_train_ds, batch_size=batch_size, shuffle=False);\n",
    "st_val_dl = DataLoader(dataset=st_val_ds, batch_size=batch_size, shuffle=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([14, 19, 12, 27, 31, 16, 29,  1])\n",
      "tensor([19, 12, 27, 31, 16, 29,  1, 20])\n"
     ]
    }
   ],
   "source": [
    "# Test that we're not shuffling\n",
    "test_iter = iter(st_train_dl)\n",
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([20,  8,  1, 15, 55, 63, 54,  1])\n",
      "tensor([ 8,  1, 15, 55, 63, 54,  1, 60])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "tensor([60, 48, 45,  1, 29, 41, 42, 42])\n",
      "tensor([48, 45,  1, 29, 41, 42, 42, 49])\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = next(test_iter)\n",
    "print(x_test.shape)\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 48, 45, 1, 29, 41, 42, 42]\n"
     ]
    }
   ],
   "source": [
    "print(text_idx[16:24])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model with vertically stacked data\n",
    "The data all looks great now. Try training the existing model and see what we get.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = CharRnn(vocab_size, emb_dim, batch_size).to(device)\n",
    "st_optimizer = torch.optim.Adam(st_model.parameters(), lr=1e-3)\n",
    "st_criterion = nn.CrossEntropyLoss();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 20, Training Loss: 2.8800, Validation Loss: 2.8394\n",
      "Epoch: 2 / 20, Training Loss: 2.5776, Validation Loss: 2.5456\n",
      "Epoch: 3 / 20, Training Loss: 2.3984, Validation Loss: 2.3800\n",
      "Epoch: 4 / 20, Training Loss: 2.2718, Validation Loss: 2.2615\n",
      "Epoch: 5 / 20, Training Loss: 2.1741, Validation Loss: 2.1707\n",
      "Epoch: 6 / 20, Training Loss: 2.0938, Validation Loss: 2.0969\n",
      "Epoch: 7 / 20, Training Loss: 2.0284, Validation Loss: 2.0379\n",
      "Epoch: 8 / 20, Training Loss: 1.9730, Validation Loss: 1.9883\n",
      "Epoch: 9 / 20, Training Loss: 1.9241, Validation Loss: 1.9453\n",
      "Epoch: 10 / 20, Training Loss: 1.8804, Validation Loss: 1.9081\n",
      "Epoch: 11 / 20, Training Loss: 1.8416, Validation Loss: 1.8765\n",
      "Epoch: 12 / 20, Training Loss: 1.8064, Validation Loss: 1.8488\n",
      "Epoch: 13 / 20, Training Loss: 1.7741, Validation Loss: 1.8243\n",
      "Epoch: 14 / 20, Training Loss: 1.7442, Validation Loss: 1.8027\n",
      "Epoch: 15 / 20, Training Loss: 1.7166, Validation Loss: 1.7832\n",
      "Epoch: 16 / 20, Training Loss: 1.6907, Validation Loss: 1.7659\n",
      "Epoch: 17 / 20, Training Loss: 1.6649, Validation Loss: 1.7511\n",
      "Epoch: 18 / 20, Training Loss: 1.6395, Validation Loss: 1.7376\n",
      "Epoch: 19 / 20, Training Loss: 1.6166, Validation Loss: 1.7275\n",
      "Epoch: 20 / 20, Training Loss: 1.5970, Validation Loss: 1.7158\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 20\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(st_model, st_optimizer, st_criterion, st_train_dl)\n",
    "    test_loss = test(st_model, st_val_dl, st_criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is training pretty nicely now. I'm seeing it max out around epoch 30 with a validation loss of 1.42 when looking at just the first book. From there the training loss keeps coming down and the validation loss starts to climb again. This gives us some clues for what to do next to improve things. However, I'll conclude this research here as I am merely using this model to help build \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get those belows rigloops govin\n",
      "\n",
      "I’m’ to be onx ither.\n",
      "\n",
      "‘AVE TH’R\n",
      "THE, Thic’ said to the Mock,\n",
      "‘Nfurh: and tarktell on a\n",
      "mance, jouse his aruse were quige.\n",
      "\n",
      "‘Go any forn they plesser!’\n",
      "\n",
      "‘Ow pit head to\n",
      "ge was eyes again, and of on OUve:’\n",
      "\n",
      "‘Of casennly so the bidger little meave dor,’ mad I Tably g ropcor siawing,’ said the Foothan house and to the blit wonder the ont,’ Alice thig; SOt, fpening. ANister, i\n"
     ]
    }
   ],
   "source": [
    "gen_text(st_model, \"Get thos\", 400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We're beginning to see real words form, which is impressive considering that this model has very little data to train on. The point of this model was to build towards an FAQ Q&A generator, which can be achieved with a few more steps that I've learned along the way. That being the use of ONMT-py, GloVe and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
