{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the default tensor type at the top\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor if torch.cuda.is_available() else \n",
    "                              torch.FloatTensor)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144348\n"
     ]
    }
   ],
   "source": [
    "#Import text data, Alice in Wonderland from local directory\n",
    "path = \"./aiw.txt\"\n",
    "\n",
    "text= open(path).read()\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, ‘and what is the use of a book,’ thought Alice ‘without pictures or\\nconversations?’\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain w'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The vocabulary is all the unique symbols used in the text. This is the benefit of \n",
    "working with a character level RNN.\"\"\"\n",
    "\n",
    "chars = sorted(set(text))\n",
    "vocab_size= len(chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '*': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " ':': 9,\n",
       " ';': 10,\n",
       " '?': 11,\n",
       " 'A': 12,\n",
       " 'B': 13,\n",
       " 'C': 14,\n",
       " 'D': 15,\n",
       " 'E': 16,\n",
       " 'F': 17,\n",
       " 'G': 18,\n",
       " 'H': 19,\n",
       " 'I': 20,\n",
       " 'J': 21,\n",
       " 'K': 22,\n",
       " 'L': 23,\n",
       " 'M': 24,\n",
       " 'N': 25,\n",
       " 'O': 26,\n",
       " 'P': 27,\n",
       " 'Q': 28,\n",
       " 'R': 29,\n",
       " 'S': 30,\n",
       " 'T': 31,\n",
       " 'U': 32,\n",
       " 'V': 33,\n",
       " 'W': 34,\n",
       " 'X': 35,\n",
       " 'Y': 36,\n",
       " 'Z': 37,\n",
       " '[': 38,\n",
       " ']': 39,\n",
       " '_': 40,\n",
       " 'a': 41,\n",
       " 'b': 42,\n",
       " 'c': 43,\n",
       " 'd': 44,\n",
       " 'e': 45,\n",
       " 'f': 46,\n",
       " 'g': 47,\n",
       " 'h': 48,\n",
       " 'i': 49,\n",
       " 'j': 50,\n",
       " 'k': 51,\n",
       " 'l': 52,\n",
       " 'm': 53,\n",
       " 'n': 54,\n",
       " 'o': 55,\n",
       " 'p': 56,\n",
       " 'q': 57,\n",
       " 'r': 58,\n",
       " 's': 59,\n",
       " 't': 60,\n",
       " 'u': 61,\n",
       " 'v': 62,\n",
       " 'w': 63,\n",
       " 'x': 64,\n",
       " 'y': 65,\n",
       " 'z': 66,\n",
       " '‘': 67,\n",
       " '’': 68,\n",
       " '“': 69,\n",
       " '”': 70}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{c:i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionaries from character --> index and index --> character\n",
    "c_to_idx= {c:i for i, c in enumerate(chars)}\n",
    "idx_to_c= {i:c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Convert whole text to indicies. Want each character to be \\nrepresented by its index in the vocabulary. This is how we will feed to RNN'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Convert whole text to indicies. Want each character to be \n",
    "represented by its index in the vocabulary. This is how we will feed to RNN\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 19, 12, 27, 31, 16, 29, 1, 20, 8]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_idx = [c_to_idx[c] for c in text]\n",
    "text_len = len(text_idx)\n",
    "text_idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on t\n",
      "--------\n",
      "t-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on t\n"
     ]
    }
   ],
   "source": [
    "#Check it works to convert back : join up the indicies\n",
    "\n",
    "print(text[25:100])\n",
    "print(\"--------\")\n",
    "print(''.join([idx_to_c[i] for i in text_idx[25:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataLoader\n",
    "#Sequence of characters passed to RNN at a time. This dictates the length of the unrolled model (#timesteps)\n",
    "#Batch size affects splitting of raw data as well as model architecture\n",
    "\n",
    "seq_len = 8\n",
    "batch_size= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wnat a non-overlapping set of inputs and outputs. Each X should be equal to the sequence length, while the Y, shifted by 1. Note that we don't go to the end for Y.\n",
    "\n",
    "idx_in_data = [text_idx[idx:idx+seq_len] for idx in range(0, text_len-1-seq_len,seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18043, 8)\n",
      "[[14 19 12 27 31 16 29  1]\n",
      " [20  8  1 15 55 63 54  1]\n",
      " [60 48 45  1 29 41 42 42]]\n"
     ]
    }
   ],
   "source": [
    "#Convert these inputs into a numpy array and provide info. Note dimensions are the total number of sequences in the corpus and the sequence length.\n",
    "\n",
    "inp = np.array(idx_in_data)\n",
    "print(inp.shape)\n",
    "print(inp[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the samething with Y\n",
    "\n",
    "idx_out_data = [text_idx[idx:idx+seq_len] for idx in range(1, text_len-seq_len, seq_len)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18043, 8)\n",
      "[[19 12 27 31 16 29  1 20]\n",
      " [ 8  1 15 55 63 54  1 60]\n",
      " [48 45  1 29 41 42 42 49]]\n"
     ]
    }
   ],
   "source": [
    "#Confirm that the target array is the input array shifted by 1. We'll be predicting the next character in sequence.\n",
    "\n",
    "outp = np.array(idx_out_data)\n",
    "print(outp.shape)\n",
    "print(outp[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split up the input and target data into training and test sets.\n",
    "Return 4 numpy arrays- training input, training targets, test input, and test targets'''\n",
    "\n",
    "def train_test_split(inp_data, out_data, train_fraction):\n",
    "    trn_idx = np.random.rand(len(inp_data)) < train_fraction\n",
    "    \n",
    "    inp_trn = inp_data[trn_idx]\n",
    "    inp_test = inp_data[~trn_idx]\n",
    "    \n",
    "    outp_trn= out_data[trn_idx]\n",
    "    outp_test= out_data[~trn_idx]\n",
    "    return inp_trn, outp_trn, inp_test, outp_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into 90%training, 10% test. This ratio should be bigger with a larger corpus.\n",
    "\n",
    "x_trn, y_trn, x_val, y_val = train_test_split(inp,outp, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PyTorch Dataset class for character level text generation. X and Y have widths equal to the sequence length'''\n",
    "\n",
    "class CharSeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        return(item,label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and validation datasets\n",
    "\n",
    "train_ds = CharSeqDataset(x_trn, y_trn)\n",
    "val_ds = CharSeqDataset(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn these into PyTorch dataloaders with batch size = bath_size.\n",
    "#This will take care of the shuffling and batching\n",
    "\n",
    "train_dl = DataLoader(dataset=train_ds, batch_size = batch_size, shuffle=True)\n",
    "val_dl = DataLoader(dataset=val_ds, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[45, 45,  8,  1, 67, 20, 68, 53],\n",
      "        [61, 54, 47, 58, 65,  1, 46, 55]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[45,  8,  1, 67, 20, 68, 53,  1],\n",
      "        [54, 47, 58, 65,  1, 46, 55, 58]])\n"
     ]
    }
   ],
   "source": [
    "'''A couple experiments with Data Loaders:\n",
    "1. The X and Y values are paired. Show that shuffling keeps them lined up.\n",
    "2. You get a different order whenever you iterate over a dataloader.'''\n",
    "exp_iter= iter(train_dl)\n",
    "x_exp, y_exp = next(exp_iter)\n",
    "\n",
    "#Exp 1\n",
    "print(x_exp.shape) # batch size by sequence length\n",
    "print(type(x_exp))\n",
    "print(x_exp[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp.shape)\n",
    "print(type(y_exp))\n",
    "print(y_exp[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[63, 49, 60, 48,  1, 60, 48, 45],\n",
      "        [53, 55, 61, 59, 45,  6,  1, 41]])\n",
      "*****\n",
      "torch.Size([512, 8])\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[49, 60, 48,  1, 60, 48, 45,  0],\n",
      "        [55, 61, 59, 45,  6,  1, 41, 46]])\n"
     ]
    }
   ],
   "source": [
    "# Exp 2.\n",
    "exp_iter2 = iter(train_dl)\n",
    "x_exp2, y_exp2 = next(exp_iter2)\n",
    "\n",
    "print(x_exp2.shape) # batch size by sequence length\n",
    "print(type(x_exp2))\n",
    "print(x_exp2[:2, :])\n",
    "print(\"*****\")\n",
    "print(y_exp2.shape)\n",
    "print(type(y_exp2))\n",
    "print(y_exp2[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN model class using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension for Character's learned embeddings. Number of hidden units in the RNN.\n",
    "emb_dim= 42\n",
    "n_hidden = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pytorch model.\n",
    "One sequence step involves embedding layer->RNN->fully connected layer->softmax over vocabulary\n",
    "A couple tricky points:\n",
    "-Want to keep the hidden activation values after a forward pass. So I have to detach h after a \n",
    "forward pass so BPTT doesn't have to go through all the steps back to the very beginning of the corpus.\n",
    "-Output predictions are rank 3 tensor of batch_size x seq_len x vocab length (it's a prediction over the vocab\n",
    "for each char in the sequence and for each sequence in the minibatch). Softmax only accepts rank 2, so need to\n",
    "reshape this into a (batch_size * seq_len) x vocab_length tensor.\n",
    "'''\n",
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, bs):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, emb_dim) # Going from vocab size down to embedding size\n",
    "        # Automatically runs for N sequence steps, which is known from input data size\n",
    "        self.rnn = nn.RNN(emb_dim, n_hidden) # embedding size to number of hidden units\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.h = self.init_h(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs.shape[0]\n",
    "        if self.h.shape[1] != bs:\n",
    "            self.h = self.init_h(bs)\n",
    "        inp = self.e(cs)\n",
    "        inp = torch.transpose(inp, 0, 1)\n",
    "        outp, h = self.rnn(inp, self.h)\n",
    "        self.h = Variable(h.data) # Save hidden values for next forward pass. Remove from BPTT by rewrapping in Var\n",
    "        outp = F.log_softmax(self.l_out(outp), dim=-1)\n",
    "        outp = torch.transpose(outp, 0, 1)\n",
    "        return outp.contiguous().view(-1, vocab_size) #This is tricky! Write myself a note it\n",
    "    \n",
    "    def init_h(self, bs):\n",
    "        return Variable(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function does 1 epoch (pass through the data)\n",
    "def train(model, opt, crit, train_loader):\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        opt.zero_grad()\n",
    "        outputs = model(inputs.to(device))\n",
    "        targets = targets.view(-1).to(device)\n",
    "        loss = crit(outputs, targets)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        losses.append(loss.data);\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function calculates average loss over all the test data.\n",
    "def test(model, test_loader, crit):\n",
    "    # Put model in evaluation mode. Read up on what it does\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs.to(device))\n",
    "            targets = targets.view(-1).to(device)\n",
    "#             l = F.nll_loss(outputs, targets, reduction='sum').item() / len(targets)# sum up batch loss\n",
    "            l = crit(outputs, targets)\n",
    "            test_loss += l.item()\n",
    "            pred = outputs.max(1, keepdim=True)[1] # get the index of the max log-probability (char index)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRnn(vocab_size, emb_dim, batch_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 4, Training Loss: 2.8930, Validation Loss: 2.8178\n",
      "Epoch: 2 / 4, Training Loss: 2.5932, Validation Loss: 2.5552\n",
      "Epoch: 3 / 4, Training Loss: 2.4434, Validation Loss: 2.4203\n",
      "Epoch: 4 / 4, Training Loss: 2.3539, Validation Loss: 2.3269\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "for ep in range(epochs):\n",
    "    tr_loss = train(model, optimizer, criterion, train_dl)\n",
    "    test_loss = test(model, val_dl, criterion)\n",
    "    print(f'Epoch: {ep+1} / {epochs}, Training Loss: {tr_loss[-1]:.4f}, Validation Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#How well did it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Given an input and a trained model, do a forward pass and predict the next character in the input sequence.\n",
    "Return this character as its integer index in the vocabulary.\n",
    "'''\n",
    "def next_letter(my_model, inp):\n",
    "\n",
    "    inp = torch.tensor([inp])\n",
    "    model_out = my_model(inp)\n",
    "    # Grab the last letter from the model output\n",
    "    # And sample from the vocabulary based on the weighted probability for character in the vocab.\n",
    "    # This makes this result non-deterministic, there can be variance between the next letter in the sequence\n",
    "    # depending on the sampling. Especially if multiple character get assigned similar probabilities.\n",
    "    next_idx = torch.multinomial(model_out[-1].exp(), 1).item()\n",
    "    \n",
    "    # return the next character index in the sequence\n",
    "    return next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 l\n"
     ]
    }
   ],
   "source": [
    "mytext = \"thos\"\n",
    "mytext = [c_to_idx[i] for i in mytext]\n",
    "nl = next_letter(model, mytext)\n",
    "print(nl, idx_to_c[nl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Keep generating the next character in the sequence. Repeatedly move the sampling window to include the latest\n",
    "prediction and predict the next letter. Goes for num_chars repetitions.\n",
    "'''\n",
    "def gen_text(my_model, inp, num_chars):\n",
    "    text = inp\n",
    "    inp = [c_to_idx[i] for i in inp]\n",
    "    for c in range(num_chars):\n",
    "        l = next_letter(my_model, inp)\n",
    "        text += idx_to_c[l]\n",
    "        inp = inp[1:]+[l]\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helloint--ove? ‘YVNam!’\n",
      "\n",
      "ures il\n",
      "ore. of en:a dot Allaryntow, madtas ce, and thir thu fadliec it af che cit ous of thedriter ab\n",
      "t:e. ans bto he ca Iinp whas oilgeet bertehyftoting wals teot. ‘Sy!’’\n",
      "\n",
      "‘I\n",
      "B Thery eupnf her’t anr herome\n",
      "\n",
      "‘Thit,’ver,AKit saidl!!: l, pthe Heare is ireen -h*o to agrin.\n",
      "‘wNoutfomu gor\n",
      "ucbynbin\n",
      "tha beab atosaingit Alihk I’ sy-e’ shatleaot, ail dfon  ailas  are onne toibt, qouse\n"
     ]
    }
   ],
   "source": [
    "gen_text(model, \"Hello\", 400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 Summary\n",
    "It looks like it's starting to work alright, especially since it hasn't trained for too long. I found that benefits of continued training started to level off after ~30 epochs or so.\n",
    "\n",
    "I learned something important here though. When I split up the corpus into sequences of length 8 (sequence length / bptt length), characters 1 - 8 are the first training example in batch 1, 9 - 16 are the second etc. What that means is that the hidden states after the forward pass are meaningless for the next batch. There's no information gained about the previous sequence to help you out with the current sequence!\n",
    "\n",
    "Here's a different idea -> What if characters 1 - 8 make up the first training example of the first batch, then characters 9 - 16 make up the first training example of the second batch. That way (since we're saving activation values) when character 9 gets passed in as the first step to the RNN, the activations correspond to what came out after character 8, which was the last character of example 1 in the previous minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
